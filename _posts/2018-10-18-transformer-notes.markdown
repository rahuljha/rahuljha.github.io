This is a living document where I am trying to organize my notes about the Transformer architecture. It's rough at the moment and will continue to be for a bit :)

# Details of the method

## Research Articles

[Original paper](https://arxiv.org/pdf/1706.03762.pdf)

Used in [BERT representations](https://arxiv.org/abs/1810.04805). 

## Blog posts

[Google's own blog article describing the architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)

[Very detailed article with lots of references for further reading](https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.W8jJjmhKhaQ)

[To Read](http://jalammar.github.io/illustrated-transformer/)

[To Read](http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/) 

# Implementations 

[Reference implementation from authors](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py)
[Nice code annotation for the paper](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

# Industry Usage


