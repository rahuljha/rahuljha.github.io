This is a living document where I am trying to organize my notes about the Transformer architecture. It's rough at the moment and will continue to be for a bit :)

# Details of the method

## Research Articles

Here's the original paper: https://arxiv.org/pdf/1706.03762.pdf

It's been used in BERT representations (https://arxiv.org/abs/1810.04805). There's probably other uses I have to find.

## Blog posts

Google's own blog: https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.W8jJjmhKhaQ

http://jalammar.github.io/illustrated-transformer/

http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/

# Implementations 

http://nlp.seas.harvard.edu/2018/04/03/attention.html

# Industry Usage

